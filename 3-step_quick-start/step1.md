[<< Return to Overview](README.md)

**STEP 1** | [STEP 2](step2.md) | [STEP 3](step3.md)

# Step 1

1. **Setup Vitis AI and run example applications with minimal dependencies**
2. Add Vitis AI Library and Model Zoo and run dependent sample applications
3. Build and run more comprehensive demonstration code
---

This step will cover installing the Vitis AI deployment/runtime environment and testing a simple application on the evaluation board.  Vitis AI can be broadly broken down into the following components:

 1. Cross-compiling SDK (sdk.sh)
    - [Xilinx@GitHub / Vitis-AI / VART / README.md > Quick Start For Edge](https://github.com/Xilinx/Vitis-AI/blob/master/VART/README.md#quick-start-for-edge)
 2. Vitis AI GitHub repository for sources
    - [https://github.com/Xilinx/Vitis-AI](https://github.com/Xilinx/Vitis-AI)
 3. Linux install packages (\*.deb) for runtime dependencies

Here we will focus on the installation and use of items (1) and (2) above.  While subsequent steps will add the use of item (3).

| Note: |
|:---|
|Vitis AI release v1.1 introduced use of the cross-compiling SDK (1) in favour of a Docker container used in earlier releases.  This is an important change and should help facilitate integration between embedded Linux development and AI deployment.  However, Docker containers continue to be used by Vitis AI for model development.

## 1.1 Setup Vitis AI
This sections covers installation of the cross-compiling SDK and cloning of the Vitis-AI repo.

### 1.1.1 Clone Repo
Start by cloning the Vitis AI GitHub repo (i.e.Xilinx@GitHub / Vitis-AI).  The repo contains the actual example source code we want to build.  Granted, it also contains a lot of other files/examples we don't need for this first step, but cloning the whole repo is simplest.
 ```
 $ git clone https://github.com/Xilinx/Vitis-AI.git
 $ cd Vitis-AI/
 $ git checkout v1.1
 ```

### 1.1.2 Install SDK
Although the term 'SDK' can be pretty generic, here it has a fairly specific origin.  The supplied SDK is just the output product of building an SDK with BitBake or Yocto.  In this case, it's an SDK generated by the Xilinx PetaLinux tool to accommodate both supported boards (ZCU102 & ZCU104).  But since PetaLinux uses Yocto as a back-end tool the supplied sdk.sh matches what one would expect of Yocto or BitBake.

For those not familiar with BitBake, the sdk.sh is used to install a cross-compiling development environment on a host machine for building Linux user-space applications.  This includes the toolchain as well as  sysroot.

To install the SDK download and follow the instructions here:
 - [Xilinx@GitHub / Vitis-AI / VART / README.md > Quick Start For Edge > Setting Up the Host](https://github.com/Xilinx/Vitis-AI/blob/master/VART/README.md#setting-up-the-host)

Here are a few helpful notes on some of the steps in the setup instructions:
 - (3) The environment script (environment-setup-aarch64-xilinx-linux) is being sourced.  For those not familiar with the syntax, the dot (.) operator is equivalent to the "source" command. 
 - (4) The provided sdk.sh was built from a generic PetaLinux project with no specific application being targeted.  As a result, the sysroot is missing files required for developing AI applications.  The additional vitis_ai_2019.2-r1.1.0.tar.gz file contains these missing files and is used to extend the baseline sysroot provided by sdk.sh.
 - (5) Similarly, the sysroot is also missing files to support using the Google logging module (glog) in applications.  In addition to installing these files to sysroot, a glog package is made for later installation on the target filesystem (i.e. SD card).

That's it! Vitis AI should now be setup on the Linux host.

## 1.2 Build Application
The "Setting Up the Host" instructions that you were directed to above actually ended by building an example application.  So you could already check this step off!  But rather than leave it at that, we'll use this section to introduce the software stack and build another (more interesting) example application to test on the board.

### 1.2.1 Software Stack

Vitis AI offers several different APIs through a collection of libraries.  A simplified diagram of how these APIs stack up is given below (a more detailed diagram can be found here: [Xilinx@GitHub / Vitis-AI / Vitis-AI-Library / README.md > Introduction](https://github.com/Xilinx/Vitis-AI/tree/f122072b2016f8dfa37d5d2cd0a82ca8a2f3f4ae/Vitis-AI-Library#introduction)).

| Application |
|:---:|
|Vitis AI Library: Model Libs (API_1/API_2)
|Vitis AI Library: Base Libs
|--> ***Vitis AI Runtime (VART): Unified API (API_0)*** <--
|Vitis AI Runtime (VART): Implementation API / Core Libs
|XRT (Xilinx RunTime)

The applications being built in this Step 1 of the tutorial use the Unified API (API_0) layer highlighted above.  This will keep things simple for now by avoiding dependencies on the higher-level libs.  All these example applications are provided as part of the Vitis AI repo and can be found here:
 - [Xilinx@GitHub / Vitis-AI / VART / samples](https://github.com/Xilinx/Vitis-AI/tree/f122072b2016f8dfa37d5d2cd0a82ca8a2f3f4ae/VART/samples)

A table describing these (7) applications can also be found here: 
 - Xilinx@GitHub / Vitis-AI / README.md > [Programming with Vitis AI](https://github.com/Xilinx/Vitis-AI/tree/v1.1#programming-with-vitis-ai)

### 1.2.2 Build Additional Example
The earlier setup steps already ended by building the `resnet50` example but that application only processes images.  To make things more interesting we'll build another example that works with video.  Here we show steps to build the `video_analysis` application:
```
$ cd <path-to-vitis-ai-repo>/VART/samples/video_analysis
$ ./build.sh
```
A new executable file should appear after the build has completed named `video_analysis`.

Since the goal of this first step is mostly just to test the setup we won't linger too much on details of the applications being built but it's worth pointing out one important fact at this point - building a Vitis-AI application will always consist of at least these two components:
 1. Host application code
 2. Compiled network/model

In the case of these examples we're only compiling the host code and then combining that binary with the binary of a pre-compiled network.  The network binary was provided as part of the example and comes from the `model...` directory.  For example:
 - `<path-to-vitis-ai-repo>/VART/samples/resnet50/model_dir_for_zcu102/resnet50.elf`
 - `<path-to-vitis-ai-repo>/VART/samples/video_analysis/model_dir_for_zcu102/ssd_traffic_pruned_0_9.elf`
 
With similar files at directories for the ZCU104.

Generating these binaries would require use of the Vitis AI development toolchain (i.e Xilinx@DockerHub/vitis-ai:tools image) which is not covered in this tutorial.

## 1.3 Board Setup
In this section we'll prepare the board, which mostly consists of preparing the SD card, and then boot it up.

### 1.3.1 Create Board SD Card
This section covers burning a pre-compiled board image to an SD card and copying over the example applications.  First download the applicable board image from this GitHub page:
 - Xilinx@GitHub / Vitis-AI / VART / README.md > [Setting Up the Target](https://github.com/Xilinx/Vitis-AI/tree/f122072b2016f8dfa37d5d2cd0a82ca8a2f3f4ae/VART#setting-up-the-target)

You could also follow the board setup instructions at the above link, which rely on a network connection to the board for file transfers, but this tutorial follows an alternative method that relies only on the SD card.

| Note: |
|:---|
|Outside of maybe some subtle differences, these board images are just standard "Vitis Embedded DPU Platforms" as can be found under [Vitis Embedded Platforms on the Xilinx Downloads page](https://www.xilinx.com/support/download/index.html/content/xilinx/en/downloadNav/embedded-platforms.html).

Next burn the image to an SD card at least 8GB in size.  On Windows this can be done with a free application such as one of the following:
 - [Win32DiskImager](https://sourceforge.net/projects/win32diskimager/)
 - [balenaEtcher](https://www.balena.io/etcher/)

Instructions using Etcher can be found in Vitis AI User Documentation (UG1431 v1.1): [Home > Vitis AI User Guide > Quick Start > Setting Up the Evaluation Board > Flashing the OS Image to the SD Card](https://www.xilinx.com/html_docs/vitis_ai/1_1/gum1570690244788.html?hl=etcher)

On Linux `dd` can be used (replacing *`<sdx>`* below with the appropriate block device):
```
$ gunzip -c xilinx-zcu104-dpu-v2019.2.img.gz | sudo dd of=/dev/<sdx> bs=1M
```
The resulting SD card will have two partitions: BOOT (FAT32) and ROOTFS (Ext4).  The BOOT partition contains bootloader firmware while the ROOTFS partition is the OS filesystem.

| Note: |
|:---|
|You may notice that the ROOTFS partition have very little free space available.  However, an init script is provided on the SD card to expand the size of this partition across all unallocated space.  This script will run the first time you power-up the board and allow you to access all available space on  your card.

### 1.3.2 Copy Files to SD Card

Now that the SD card is ready we just need to copy a few items to the card from the host:

 1. Example applications
 2. Application input data
 3. Vitis AI Runtime (VART) Libraries
 4. Glog package (built earlier)

The BOOT partition will be used to hold these items mostly because it has more space than the ROOTFS partition, but it's also accessible from a Windows machine.

To make this step as simple as possible we'll just copy the entire sample directory over to the SD card.  For example:

```
$ cp -r <path-to-vitis-ai-repo>/VART/samples <path-to-mounts>/BOOT/
```

Example input data (images & videos) is provided by Xilinx and can be downloaded from here:
 - [https://www.xilinx.com/bin/public/openDownload?filename=vitis_ai_runtime_r1.1_image_video.tar.gz](https://www.xilinx.com/bin/public/openDownload?filename=vitis_ai_runtime_r1.1_image_video.tar.gz)

A link is also available in the GitHub documentation: [Xilinx@GitHub / Vitis-AI / VART / README.md > Running Vitis AI Examples (For Edge)](https://github.com/Xilinx/Vitis-AI/tree/f122072b2016f8dfa37d5d2cd0a82ca8a2f3f4ae/VART#running-vitis-ai-examples-for-edge)

Once you have the downloaded the archive of sample input data extract it on the host machine and then copy it to the SD card.  Here's an example of the overall steps on Linux:

```
$ cd <path-to-mounts>/BOOT
$ wget -O vitis_ai_runtime_r1.1_image_video.tar.gz https://www.xilinx.com/bin/public/openDownload?filename=vitis_ai_runtime_r1.1_image_video.tar.gz
```

As was shown in the stack diagram in section 1.2.1 above, the applications we're going to run depend only on VART and XRT.  Since XRT is pre-installed on the provided Vitis Embedded DPU Platform image we only need to add the VART libs.

The runtime libs can be downloaded from here:
 - [https://www.xilinx.com/bin/public/openDownload?filename=vitis-ai-runtime-1.1.2.tar.gz](https://www.xilinx.com/bin/public/openDownload?filename=vitis-ai-runtime-1.1.2.tar.gz)

A link is also available in the GitHub documentation: [Xilinx@GitHub / Vitis-AI / VART / README.md > Setting Up the Target > 2. Installing Vitis AI Runtime Package](https://github.com/Xilinx/Vitis-AI/tree/f122072b2016f8dfa37d5d2cd0a82ca8a2f3f4ae/VART#setting-up-the-target)

Here's an example of the steps:

```
$ cd <path-to-mounts>/BOOT
$ wget -O vitis-ai-runtime-1.1.2.tar.gz https://www.xilinx.com/bin/public/openDownload?filename=vitis-ai-runtime-1.1.2.tar.gz
```

Lastly, copy the glog package that was built earlier in step 1.1.2 to the SD card :

```
$ cp glog-v0.4.0.tar.gz <path-to-mounts>/BOOT
```

### 1.3.3 Setup & Connect to Board

Setup the board as follows:

 1. Inserting the SD card into the board SD slot
 2. Connecting board power
 3. Connecting the Micro-USB cable for board UART
    - For a Windows host you may need to install the appropriate USB to UART bridge driver on first-time use.  See [AR# 33569](https://www.xilinx.com/support/answers/33569.html) for more info.
    - ZCU102 boards have two Micro-USB ports so make sure you use the one labelled 'UART' (as oppposed to the one labelled 'JTAG')

If you have any optional hardware it can also be connected at this time.    

 - An Ethernet cable will provide a network connection between the board and the host computer.
 - A DisplayPort monitor and cable will provide graphical output.
 - A USB mouse and keyboard connected to the board through a USB hub (together with the monitor) will allow the board to be used standalone (i.e. without the need for a host).

| Note: |
|:---|
|Additional instructions on board setup can be found in UG1431 (v1.1) here: [Home > Vitis AI User Guide > Quick Start > Setting Up the Evaluation Board > Setting Up the ZCU102/104 Evaluation Board](https://www.xilinx.com/html_docs/vitis_ai/1_1/wld1570690239659.html?hl=setting%2Cup%2Ceval%2Cboard).

Before even powering on the board you can first connect to it over UART from the host machine (this is because the UART interface is powered via USB).  If using the board standalone this isn't strictly necessary but still nice to have.  The host should use the first interface made available by the USB-to-UART bridge (e.g. Interface 0 on Windows) with the following parameters:
 - baud rate: 115200
 - data bit: 8
 - stop bit: 1
 - no parity

On Windows, open the Device Manager to map Ports > Interface 0 to an actual COM port #.  A program like PuTTY or Tera Term can be used to establish the connection.

| Note: |
|:---|
|Additional instructions on board connectivity can be found in UG1431 (v1.1) here: [Home > Vitis AI User Guide > Quick Start > Setting Up the Evaluation Board > Accessing the Evaluation Board](https://www.xilinx.com/html_docs/vitis_ai/1_1/aeu1570690259531.html).

Power on the board using the sliding power switch and you should immediately see output on the terminal.  For example:

```
Xilinx Zynq MP First Stage Boot Loader
Release 2019.2   Nov 22 2019  -  19:57:22
NOTICE:  ATF running on XCZU9EG/silicon v4/RTL5.1 at 0xfffea000
NOTICE:  BL31: Secure code at 0x0
NOTICE:  BL31: Non secure code at 0x10080000
NOTICE:  BL31: v2.0(release):xilinx-v2019.1-12-g713dace9
NOTICE:  BL31: Built : 11:09:52, Nov 22 2019
PMUFW:  v1.1


U-Boot 2019.01 (Nov 22 2019 - 11:11:36 +0000)

.
.
.

mount: /mnt: /dev/mmcblk0p1 already mounted on /run/media/mmcblk0p1.
attempting to run /mnt/init.sh
/mnt
root@xilinx-zcu102-2019_2:~#
```

Unless the board is connected to a network/host with a DHCP server, the boot process will be a little slow as U-boot and Linux timeout waiting for an address assignment.  If using the board standalone, you will eventually see a desktop interface appear on the DisplayPort monitor.

From here forward you can use either the UART terminal interface to enter commands from the host or open the xfce4-terminal application (top left-most icon) from the desktop interface to enter commands directly on the board when used standalone.

## 1.4 Running the Example

In this section we get down to business and finally run some apps.

### 1.4.1 Install Runtime Libs

Now that the board is running we have a little more setup to finish off.  Recall that the provided SD card image is fairly generic and therefore lacking some dependencies for the example applications.  We copied these missing dependencies to the BOOT partition and now we just need to install them to the OS partition (ROOTFS).  Start by installing the glog package:
```
$ tar -xzvf /mnt/glog-v0.4.0.tar.gz --strip-components=1 -C /usr
```
Now install all the runtime dependencies:
```
$ cd # just making sure you're home
$ tar -xaf /mnt/vitis-ai-runtime-1.1.2.tar.gz
$ dpkg -i --force-all vitis-ai-runtime-1.1.2/unilog/aarch64/libunilog-1.1.0-Linux-build46.deb
$ dpkg -i vitis-ai-runtime-1.1.2/XIR/aarch64/libxir-1.1.0-Linux-build46.deb
$ dpkg -i vitis-ai-runtime-1.1.2/VART/aarch64/libvart-1.1.0-Linux-build48.deb
```
Lastly, copy over the sample applications and input data:
```
$ cp -r /mnt/samples ~/
$ tar -xaf /mnt/vitis_ai_runtime_r1.1_image_video.tar.gz
```

From here onwards we'll also assume use of a ZCU102 board so syntax can be simplified from `zcu<102|104>` to just `zcu102`.  If you have a ZCU104 board just make the appropriate replacement.

### 1.4.2 Run Terminal Example

Time to test things out.  We'll start with a Python example because it's the most fail-safe; it doesn't rely on any input/output devices and doesn't need to be built.  Here we run the Python Inception example which only requires a sample input image:
```
$ cd ~/samples/inception_v1_mt_py/
$ python3 inception_v1.py 4 model_dir_for_zcu102
215.75 FPS
```
The argument '4' sets the number of threads to use (e.g. 1, 2, 4) and the last argument must be the path to a directory containing a `meta.json` file.  Running the example without arguments will also provide a description.

| Note: |
|:---|
|The meta.json file provides runtime parameters and is generated by Vitis AI development tools not covered in this tutorial.

The Python examples only output the measured framerate (i.e. rate of frames processed by the DPU).  This is good for a terminal-only setup, but much less interesting than seeing graphical results.  The remaining examples will utilize the DisplayPort monitor (if available).

### 1.4.3 Run Image Example

If you are using a terminal connection you will need to set an environment variable to tell X11 to target the local display.  If using the board standalone, you should skip this.
```
$ export DISPLAY=:0.0 # Not needed for standalone board use
```
Now let's run the ResNet50 example.  This also requires the sample images so if you haven't already copied these to the appropriate location refer to that step in the previous section.
```
$ cd ~/samples/resnet50/
$ ./resnet50 model_dir_for_zcu102
```
The program will display each image being processed on the monitor (if available) and provide results on the terminal such as:
```
Image : 001.jpg
top[0] prob = 0.980779  name = brain coral
top[1] prob = 0.008485  name = coral reef
top[2] prob = 0.008485  name = jackfruit, jak, jack
top[3] prob = 0.000542  name = sea urchin
top[4] prob = 0.000422  name = puffer, pufferfish, blowfish, globefish

```
As seen above, the single image that was provided was classified as "brain coral" with a probability of ~98%.  If you had a DisplayPort monitor connected you should also see the processed image rendered on the display while the program is running.

---
#### Bonus Section
This section will show how you can download other images used for neural network testing/training from a standard database (ImageNet).  If you're not interested in this just skip ahead to the last video example.

A database of thousands of categorized images is maintained by [ImageNet](http://www.image-net.org/).  You can retrieve the URLs to these images and try downloading them for testing.  I say try because URLs can break after being entered into the database.  To browse the database use the [explore](http://image-net.org/explore) option on the ImageNet site.  Once you have a desired category selected use the Downloads tab to acquire a list of URLs in a text file.  Here's an example using the "Domestic animal, domesticated animal" category that you can run on a board with internet connectivity:
```
$ ifconfig eth0 up # connect to local network with DHCP
$ cd
$ # Download URL list (link provided under Downloads tab of ImageNet explorer)
$ wget -O animal_urls http://image-net.org/api/text/imagenet.synset.geturls?wnid=n02512053
$ cd ~/samples/images/
$ # Attempt to download first 5 images in URL list
$ head -n 5 ~/animal_urls | wget --no-check-certificate -i -
$ # Now run resent50 application using additional images
$ cd ~/samples/resnet50
$ ./resnet50 model_dir_for_zcu102
.
.
.
Image : 3171902677_c5a529cc1c.jpg
top[0] prob = 0.869145  name = sorrel
top[1] prob = 0.043272  name = worm fence, snake fence, snake-rail fence, Virginia fence
top[2] prob = 0.009655  name = orangutan, orang, orangutang, Pongo pygmaeus
top[3] prob = 0.005856  name = groenendael
top[4] prob = 0.004561  name = chain saw, chainsaw
.
.
.
```
---

### 1.4.4 Run Video Example

Lastly, we'll run a video example:
```
$ cd ~/samples/video_analysis/
$ ./video_analysis video/structure.mp4 model_dir_for_zcu102
```
This will display a traffic scene video for approximately 30 seconds (or you can stop the application with Ctrl+C).  The video shows the result of 3-class object detection using SSD by adding bounding boxes around the detected class (vehicle, cyclist, pedestrian).

### 1.4.5 Build and Run Other Examples

If you'd like to run other examples, you don't actually have to return to the host system to cross-compile them.  The provided SD card image is setup for self-hosting so you can compile examples locally.  Just browse to the desired example and run the build script.  For example:
```
$ cd ~/samples/adas_detection/
$ ./build.sh
$ ./adas_detection video/adas.avi model_dir_for_zcu102
```
This runs an example similar to `video_analysis`, except Yolo-v3 is used.

For information about the format of other commands, simply run the application without arguments.

This completes Step 1 of this tutorial and you're invited to explore the example code to see how the Vitis AI Unified API (API0) can be used for your own application.  For Step 2 we'll run examples that move up the software stack by leveraging higher level Vitis AI Libraries.

**STEP 1** | [STEP 2](step2.md) | [STEP 3](step3.md)
